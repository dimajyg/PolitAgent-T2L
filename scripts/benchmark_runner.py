#!/usr/bin/env python3
"""
–°–∏—Å—Ç–µ–º–∞ –±–µ–Ω—á–º–∞—Ä–∫–∏–Ω–≥–∞ LLM –º–æ–¥–µ–ª–µ–π –Ω–∞ –≤—Å–µ—Ö –∏–≥—Ä–∞—Ö PolitAgent environments.

–≠—Ç–æ—Ç –º–æ–¥—É–ª—å –ø–æ–∑–≤–æ–ª—è–µ—Ç:
- –ó–∞–ø—É—Å–∫–∞—Ç—å –º–Ω–æ–∂–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–µ–π –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ –Ω–∞ –≤—Å–µ—Ö –∏–≥—Ä–∞—Ö
- –°–æ–±–∏—Ä–∞—Ç—å –º–µ—Ç—Ä–∏–∫–∏ –ø–æ –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏ –∏ –∏–≥—Ä–µ
- –í—ã—á–∏—Å–ª—è—Ç—å –æ–±—â–∏–π —Ä–µ–π—Ç–∏–Ω–≥ –º–æ–¥–µ–ª–µ–π
- –ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –æ—Ç—á–µ—Ç—ã
"""

import asyncio
import concurrent.futures
import json
import os
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
import logging
from dataclasses import dataclass, asdict
import traceback

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –≤ –ø—É—Ç—å
sys.path.append(str(Path(__file__).parent.parent))

from llm.models import get_model, AVAILABLE_MODELS
from metrics import METRICS_MAP
from environments.diplomacy_game.game import DiplomacyGame
from environments.beast.game import BeastGame
from environments.spyfall.game import SpyfallGame
from environments.askguess.game import AskGuessGame
from environments.tofukingdom.game import TofuKingdomGame

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('benchmark.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

@dataclass
class ModelConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ –¥–ª—è –±–µ–Ω—á–º–∞—Ä–∫–∞."""
    provider: str  # 'openai', 'mistral', 'ollama'
    model_name: str  # –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∞—è –º–æ–¥–µ–ª—å
    display_name: str  # –æ—Ç–æ–±—Ä–∞–∂–∞–µ–º–æ–µ –∏–º—è
    temperature: float = 0.7
    api_key: Optional[str] = None
    max_tokens: Optional[int] = None
    enabled: bool = True

@dataclass
class GameConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∏–≥—Ä—ã –¥–ª—è –±–µ–Ω—á–º–∞—Ä–∫–∞."""
    name: str
    game_class: Any
    num_games: int = 3  # –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏–≥—Ä –¥–ª—è —É—Å—Ä–µ–¥–Ω–µ–Ω–∏—è
    max_rounds: int = 10
    enabled: bool = True

@dataclass
class BenchmarkResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –±–µ–Ω—á–º–∞—Ä–∫–∞ –æ–¥–Ω–æ–π –º–æ–¥–µ–ª–∏ –Ω–∞ –æ–¥–Ω–æ–π –∏–≥—Ä–µ."""
    model_name: str
    game_name: str
    game_results: List[Dict[str, Any]]
    metrics: Dict[str, Any]
    execution_time: float
    success_rate: float
    error_count: int
    timestamp: str

@dataclass
class ModelRating:
    """–†–µ–π—Ç–∏–Ω–≥ –º–æ–¥–µ–ª–∏."""
    model_name: str
    overall_score: float
    game_scores: Dict[str, float]
    total_games: int
    success_rate: float
    avg_execution_time: float
    rank: int = 0

class BenchmarkRunner:
    """–û—Å–Ω–æ–≤–Ω–æ–π –∫–ª–∞—Å—Å –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –±–µ–Ω—á–º–∞—Ä–∫–æ–≤."""
    
    def __init__(self, results_dir: str = "benchmark_results"):
        self.results_dir = Path(results_dir)
        self.results_dir.mkdir(exist_ok=True)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏–≥—Ä
        self.games = {
            "diplomacy": GameConfig("diplomacy", DiplomacyGame, num_games=2, max_rounds=5),
            "beast": GameConfig("beast", BeastGame, num_games=3, max_rounds=8),
            "spyfall": GameConfig("spyfall", SpyfallGame, num_games=3, max_rounds=6),
            "askguess": GameConfig("askguess", AskGuessGame, num_games=4, max_rounds=10),
            "tofukingdom": GameConfig("tofukingdom", TofuKingdomGame, num_games=3, max_rounds=8)
        }
        
        # –î–µ—Ñ–æ–ª—Ç–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        self.models = self._get_default_models()
        
        # –í–µ—Å–∞ –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ –æ–±—â–µ–≥–æ —Ä–µ–π—Ç–∏–Ω–≥–∞
        self.game_weights = {
            "diplomacy": 0.25,    # –°–ª–æ–∂–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–∞—è –∏–≥—Ä–∞
            "beast": 0.20,        # –°–æ—Ü–∏–∞–ª—å–Ω–∞—è –¥–µ–¥—É–∫—Ü–∏—è
            "spyfall": 0.20,      # –ë—ã—Å—Ç—Ä–∞—è –¥–µ–¥—É–∫—Ü–∏—è
            "askguess": 0.15,     # –õ–æ–≥–∏—á–µ—Å–∫–∏–µ –≤–æ–ø—Ä–æ—Å—ã
            "tofukingdom": 0.20   # –†–æ–ª–µ–≤–∞—è –¥–µ–¥—É–∫—Ü–∏—è
        }
    
    def _get_default_models(self) -> List[ModelConfig]:
        """–ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è."""
        models = []
        
        # OpenAI –º–æ–¥–µ–ª–∏
        if os.getenv("OPENAI_API_KEY"):
            models.extend([
                ModelConfig("openai", "gpt-3.5-turbo", "GPT-3.5 Turbo"),
                ModelConfig("openai", "gpt-4-turbo", "GPT-4 Turbo", temperature=0.7),
            ])
        
        # Mistral –º–æ–¥–µ–ª–∏
        if os.getenv("MISTRAL_API_KEY"):
            models.extend([
                ModelConfig("mistral", "mistral-tiny", "Mistral Tiny"),
                ModelConfig("mistral", "mistral-small", "Mistral Small"),
            ])
        
        # Ollama –º–æ–¥–µ–ª–∏ (–µ—Å–ª–∏ –ª–æ–∫–∞–ª—å–Ω–æ –∑–∞–ø—É—â–µ–Ω)
        try:
            import requests
            response = requests.get("http://localhost:11434/api/tags", timeout=2)
            if response.status_code == 200:
                ollama_models = response.json().get('models', [])
                available_names = [m['name'].split(':')[0] for m in ollama_models]
                
                for model_name in ['llama2', 'mistral', 'phi2', 'gemma']:
                    if model_name in available_names:
                        models.append(ModelConfig("ollama", model_name, f"Ollama {model_name.title()}"))
        except:
            logger.warning("Ollama –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –ª–æ–∫–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏")
        
        if not models:
            logger.warning("–ù–µ –Ω–∞–π–¥–µ–Ω–æ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π! –î–æ–±–∞–≤—å—Ç–µ API –∫–ª—é—á–∏ –∏–ª–∏ –∑–∞–ø—É—Å—Ç–∏—Ç–µ Ollama")
            # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ—Å—Ç–æ–≤—É—é –∑–∞–≥–ª—É—à–∫—É
            models.append(ModelConfig("test", "dummy", "Test Dummy Model", enabled=False))
        
        return models
    
    def add_model(self, provider: str, model_name: str, display_name: str, **kwargs) -> None:
        """–î–æ–±–∞–≤–ª—è–µ—Ç –º–æ–¥–µ–ª—å –≤ –±–µ–Ω—á–º–∞—Ä–∫."""
        self.models.append(ModelConfig(provider, model_name, display_name, **kwargs))
    
    def configure_game(self, game_name: str, **kwargs) -> None:
        """–ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–≥—Ä—ã."""
        if game_name in self.games:
            for key, value in kwargs.items():
                if hasattr(self.games[game_name], key):
                    setattr(self.games[game_name], key, value)
    
    async def run_single_game(self, model_config: ModelConfig, game_config: GameConfig, 
                             game_index: int) -> Tuple[Dict[str, Any], float]:
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç –æ–¥–Ω—É –∏–≥—Ä—É –¥–ª—è –æ–¥–Ω–æ–π –º–æ–¥–µ–ª–∏."""
        start_time = time.time()
        
        try:
            # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å
            model = get_model(
                model_config.provider,
                specific_model=model_config.model_name,
                temperature=model_config.temperature,
                api_key=model_config.api_key
            )
            
            # –°–æ–∑–¥–∞–µ–º –∞—Ä–≥—É–º–µ–Ω—Ç—ã –¥–ª—è –∏–≥—Ä—ã
            class Args:
                def __init__(self):
                    self.max_rounds = game_config.max_rounds
                    self.debug = False
                    self.num_players = 4  # –¥–µ—Ñ–æ–ª—Ç –¥–ª—è –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–∞ –∏–≥—Ä
            
            args = Args()
            
            # –ó–∞–ø—É—Å–∫–∞–µ–º –∏–≥—Ä—É
            game = game_config.game_class(args, model)
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∏–≥—Ä—É
            game.init_game()
            
            # –ó–∞–ø—É—Å–∫–∞–µ–º –∏–≥—Ä–æ–≤–æ–π —Ü–∏–∫–ª
            result = game.game_loop()
            
            execution_time = time.time() - start_time
            
            # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            result.update({
                "model_provider": model_config.provider,
                "model_name": model_config.model_name,
                "game_name": game_config.name,
                "game_index": game_index,
                "execution_time": execution_time,
                "timestamp": datetime.now().isoformat()
            })
            
            return result, execution_time
            
        except Exception as e:
            execution_time = time.time() - start_time
            logger.error(f"–û—à–∏–±–∫–∞ –≤ –∏–≥—Ä–µ {game_config.name} —Å –º–æ–¥–µ–ª—å—é {model_config.display_name}: {e}")
            logger.error(traceback.format_exc())
            
            return {
                "error": str(e),
                "model_provider": model_config.provider,
                "model_name": model_config.model_name,
                "game_name": game_config.name,
                "game_index": game_index,
                "execution_time": execution_time,
                "timestamp": datetime.now().isoformat()
            }, execution_time
    
    async def benchmark_model_on_game(self, model_config: ModelConfig, 
                                    game_config: GameConfig) -> BenchmarkResult:
        """–ë–µ–Ω—á–º–∞—Ä–∫ –æ–¥–Ω–æ–π –º–æ–¥–µ–ª–∏ –Ω–∞ –æ–¥–Ω–æ–π –∏–≥—Ä–µ."""
        logger.info(f"–ó–∞–ø—É—Å–∫ –±–µ–Ω—á–º–∞—Ä–∫–∞: {model_config.display_name} –Ω–∞ {game_config.name}")
        
        game_results = []
        total_time = 0
        error_count = 0
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –∏–≥—Ä –¥–ª—è —É—Å—Ä–µ–¥–Ω–µ–Ω–∏—è
        for i in range(game_config.num_games):
            result, exec_time = await self.run_single_game(model_config, game_config, i)
            game_results.append(result)
            total_time += exec_time
            
            if "error" in result:
                error_count += 1
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏–≥—Ä
        game_results_file = (self.results_dir / 
                           f"{model_config.provider}_{model_config.model_name}_{game_config.name}_games.json")
        with open(game_results_file, 'w', encoding='utf-8') as f:
            json.dump(game_results, f, indent=2, ensure_ascii=False)
        
        # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
        try:
            metrics_class = METRICS_MAP[game_config.name]
            metrics = metrics_class()
            metrics_data = metrics.calculate_metrics(str(game_results_file.parent))
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
            metrics_file = (self.results_dir / 
                          f"{model_config.provider}_{model_config.model_name}_{game_config.name}_metrics.json")
            with open(metrics_file, 'w', encoding='utf-8') as f:
                json.dump(metrics_data, f, indent=2, ensure_ascii=False)
                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≤ —Ä–∞—Å—á–µ—Ç–µ –º–µ—Ç—Ä–∏–∫ –¥–ª—è {game_config.name}: {e}")
            metrics_data = {"error": str(e)}
        
        # –í—ã—á–∏—Å–ª—è–µ–º success rate
        successful_games = len([r for r in game_results if "error" not in r])
        success_rate = successful_games / len(game_results)
        
        return BenchmarkResult(
            model_name=model_config.display_name,
            game_name=game_config.name,
            game_results=game_results,
            metrics=metrics_data,
            execution_time=total_time,
            success_rate=success_rate,
            error_count=error_count,
            timestamp=datetime.now().isoformat()
        )
    
    async def run_benchmark(self, max_parallel_models: int = 3, 
                          max_parallel_games: int = 2) -> List[BenchmarkResult]:
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç –ø–æ–ª–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ –≤—Å–µ—Ö –∏–≥—Ä–∞—Ö."""
        logger.info("–ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ –±–µ–Ω—á–º–∞—Ä–∫–∞")
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º –≤–∫–ª—é—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∏ –∏–≥—Ä—ã
        active_models = [m for m in self.models if m.enabled]
        active_games = [g for g in self.games.values() if g.enabled]
        
        logger.info(f"–¢–µ—Å—Ç–∏—Ä—É–µ–º {len(active_models)} –º–æ–¥–µ–ª–µ–π –Ω–∞ {len(active_games)} –∏–≥—Ä–∞—Ö")
        
        all_results = []
        
        # –°–µ–º–∞—Ñ–æ—Ä—ã –¥–ª—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ—Å—Ç–∏
        model_semaphore = asyncio.Semaphore(max_parallel_models)
        game_semaphore = asyncio.Semaphore(max_parallel_games)
        
        async def run_model_benchmark(model: ModelConfig) -> List[BenchmarkResult]:
            async with model_semaphore:
                model_results = []
                
                # –ó–∞–ø—É—Å–∫–∞–µ–º –≤—Å–µ –∏–≥—Ä—ã –¥–ª—è –æ–¥–Ω–æ–π –º–æ–¥–µ–ª–∏
                tasks = []
                for game in active_games:
                    async def run_game_with_semaphore(m=model, g=game):
                        async with game_semaphore:
                            return await self.benchmark_model_on_game(m, g)
                    tasks.append(run_game_with_semaphore())
                
                game_results = await asyncio.gather(*tasks, return_exceptions=True)
                
                for result in game_results:
                    if isinstance(result, Exception):
                        logger.error(f"–û—à–∏–±–∫–∞ –≤ –±–µ–Ω—á–º–∞—Ä–∫–µ –º–æ–¥–µ–ª–∏ {model.display_name}: {result}")
                    else:
                        model_results.append(result)
                
                return model_results
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –±–µ–Ω—á–º–∞—Ä–∫–∏ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π
        model_tasks = [run_model_benchmark(model) for model in active_models]
        all_model_results = await asyncio.gather(*model_tasks, return_exceptions=True)
        
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        for model_results in all_model_results:
            if isinstance(model_results, Exception):
                logger.error(f"–û—à–∏–±–∫–∞ –≤ –±–µ–Ω—á–º–∞—Ä–∫–µ –º–æ–¥–µ–ª–∏: {model_results}")
            else:
                all_results.extend(model_results)
        
        return all_results
    
    def calculate_model_ratings(self, benchmark_results: List[BenchmarkResult]) -> List[ModelRating]:
        """–í—ã—á–∏—Å–ª—è–µ—Ç —Ä–µ–π—Ç–∏–Ω–≥ –º–æ–¥–µ–ª–µ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –±–µ–Ω—á–º–∞—Ä–∫–∞."""
        logger.info("–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ä–µ–π—Ç–∏–Ω–≥–æ–≤ –º–æ–¥–µ–ª–µ–π")
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –º–æ–¥–µ–ª—è–º
        model_results = {}
        for result in benchmark_results:
            if result.model_name not in model_results:
                model_results[result.model_name] = []
            model_results[result.model_name].append(result)
        
        ratings = []
        
        for model_name, results in model_results.items():
            game_scores = {}
            total_success_rate = 0
            total_execution_time = 0
            total_games = 0
            
            for result in results:
                game_name = result.game_name
                
                # –ë–∞–∑–æ–≤—ã–π —Å–∫–æ—Ä –Ω–∞ –æ—Å–Ω–æ–≤–µ success rate
                base_score = result.success_rate * 100
                
                # –ë–æ–Ω—É—Å—ã –∑–∞ –∫–∞—á–µ—Å—Ç–≤–æ –∏–∑ –º–µ—Ç—Ä–∏–∫
                quality_bonus = 0
                if "model_performance" in result.metrics:
                    perf = result.metrics["model_performance"]
                    if "average_quality_score" in perf:
                        quality_bonus += perf["average_quality_score"] * 20  # –¥–æ 20 –æ—á–∫–æ–≤
                    if "decision_consistency" in perf:
                        quality_bonus += perf["decision_consistency"] * 10  # –¥–æ 10 –æ—á–∫–æ–≤
                
                # –ë–æ–Ω—É—Å –∑–∞ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å (–±—ã—Å—Ç—Ä–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ)
                efficiency_bonus = 0
                if result.execution_time > 0:
                    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤—Ä–µ–º—è (–º–µ–Ω—å—à–µ –≤—Ä–µ–º–µ–Ω–∏ = –±–æ–ª—å—à–µ –±–æ–Ω—É—Å)
                    max_time = 300  # 5 –º–∏–Ω—É—Ç –º–∞–∫—Å–∏–º—É–º
                    efficiency_bonus = max(0, (max_time - result.execution_time) / max_time * 10)
                
                # –®—Ç—Ä–∞—Ñ –∑–∞ –æ—à–∏–±–∫–∏
                error_penalty = result.error_count * 10
                
                # –ò—Ç–æ–≥–æ–≤—ã–π —Å–∫–æ—Ä –¥–ª—è –∏–≥—Ä—ã
                game_score = max(0, base_score + quality_bonus + efficiency_bonus - error_penalty)
                game_scores[game_name] = game_score
                
                total_success_rate += result.success_rate
                total_execution_time += result.execution_time
                total_games += 1
            
            # –í–∑–≤–µ—à–µ–Ω–Ω—ã–π –æ–±—â–∏–π —Å–∫–æ—Ä
            overall_score = sum(game_scores.get(game, 0) * weight 
                              for game, weight in self.game_weights.items())
            
            avg_success_rate = total_success_rate / len(results) if results else 0
            avg_execution_time = total_execution_time / len(results) if results else 0
            
            ratings.append(ModelRating(
                model_name=model_name,
                overall_score=overall_score,
                game_scores=game_scores,
                total_games=total_games,
                success_rate=avg_success_rate,
                avg_execution_time=avg_execution_time
            ))
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –æ–±—â–µ–º—É —Å–∫–æ—Ä—É –∏ –ø—Ä–∏—Å–≤–∞–∏–≤–∞–µ–º —Ä–∞–Ω–∫–∏
        ratings.sort(key=lambda r: r.overall_score, reverse=True)
        for i, rating in enumerate(ratings):
            rating.rank = i + 1
        
        return ratings
    
    def generate_benchmark_report(self, benchmark_results: List[BenchmarkResult], 
                                ratings: List[ModelRating]) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∏—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á–µ—Ç –ø–æ –±–µ–Ω—á–º–∞—Ä–∫—É."""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        report = f"""# PolitAgent Benchmark Report
Generated: {timestamp}

## Executive Summary

–ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–æ **{len(set(r.model_name for r in benchmark_results))}** –º–æ–¥–µ–ª–µ–π –Ω–∞ **{len(set(r.game_name for r in benchmark_results))}** –∏–≥—Ä–∞—Ö.

### Top 3 Models:
"""
        
        for i, rating in enumerate(ratings[:3]):
            report += f"{i+1}. **{rating.model_name}** - {rating.overall_score:.1f} –æ—á–∫–æ–≤\n"
        
        report += f"\n## Detailed Rankings\n\n"
        report += "| Rank | Model | Overall Score | Success Rate | Avg Time |\n"
        report += "|------|-------|---------------|--------------|----------|\n"
        
        for rating in ratings:
            report += (f"| {rating.rank} | {rating.model_name} | "
                      f"{rating.overall_score:.1f} | {rating.success_rate:.1%} | "
                      f"{rating.avg_execution_time:.1f}s |\n")
        
        # –î–µ—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –∏–≥—Ä–∞–º
        report += f"\n## Performance by Game\n\n"
        
        for game_name in self.game_weights.keys():
            report += f"### {game_name.title()}\n\n"
            report += "| Rank | Model | Score | Success Rate |\n"
            report += "|------|-------|-------|-------------|\n"
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –º–æ–¥–µ–ª–∏ –ø–æ —Å–∫–æ—Ä—É –≤ —ç—Ç–æ–π –∏–≥—Ä–µ
            game_ratings = [(r.model_name, r.game_scores.get(game_name, 0), 
                           next((br.success_rate for br in benchmark_results 
                                if br.model_name == r.model_name and br.game_name == game_name), 0))
                          for r in ratings]
            game_ratings.sort(key=lambda x: x[1], reverse=True)
            
            for i, (model, score, success_rate) in enumerate(game_ratings):
                report += f"| {i+1} | {model} | {score:.1f} | {success_rate:.1%} |\n"
            
            report += "\n"
        
        # –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        report += "## Analysis\n\n"
        
        # –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å –≤ —Ü–µ–ª–æ–º
        if ratings:
            best_model = ratings[0]
            report += f"**Best Overall Model**: {best_model.model_name} with {best_model.overall_score:.1f} points\n"
            
            # –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å –ø–æ –∏–≥—Ä–∞–º
            for game_name in self.game_weights.keys():
                game_ratings = [(r.model_name, r.game_scores.get(game_name, 0)) for r in ratings]
                game_ratings.sort(key=lambda x: x[1], reverse=True)
                if game_ratings:
                    report += f"**Best at {game_name.title()}**: {game_ratings[0][0]} ({game_ratings[0][1]:.1f} points)\n"
        
        return report
    
    def save_results(self, benchmark_results: List[BenchmarkResult], 
                    ratings: List[ModelRating]) -> Tuple[str, str]:
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –±–µ–Ω—á–º–∞—Ä–∫–∞."""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º JSON —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
        results_file = self.results_dir / f"benchmark_results_{timestamp}.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            data = {
                "timestamp": timestamp,
                "benchmark_results": [asdict(r) for r in benchmark_results],
                "model_ratings": [asdict(r) for r in ratings],
                "game_weights": self.game_weights
            }
            json.dump(data, f, indent=2, ensure_ascii=False)
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—á–µ—Ç
        report = self.generate_benchmark_report(benchmark_results, ratings)
        report_file = self.results_dir / f"benchmark_report_{timestamp}.md"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
        
        return str(results_file), str(report_file)

# –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è CLI
async def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –±–µ–Ω—á–º–∞—Ä–∫–∞."""
    runner = BenchmarkRunner()
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –±–µ–Ω—á–º–∞—Ä–∫–∞ (–º–æ–∂–Ω–æ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å)
    runner.configure_game("diplomacy", num_games=1, max_rounds=3)  # –ë—ã—Å—Ç—Ä—ã–π —Ç–µ—Å—Ç
    runner.configure_game("beast", num_games=2, max_rounds=5)
    runner.configure_game("spyfall", num_games=2, max_rounds=4)
    runner.configure_game("askguess", num_games=2, max_rounds=8)
    runner.configure_game("tofukingdom", num_games=2, max_rounds=6)
    
    print("üöÄ –ó–∞–ø—É—Å–∫ PolitAgent Benchmark")
    print(f"üìä –ú–æ–¥–µ–ª–µ–π –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: {len([m for m in runner.models if m.enabled])}")
    print(f"üéÆ –ò–≥—Ä –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: {len([g for g in runner.games.values() if g.enabled])}")
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –±–µ–Ω—á–º–∞—Ä–∫
    start_time = time.time()
    benchmark_results = await runner.run_benchmark(max_parallel_models=2, max_parallel_games=1)
    total_time = time.time() - start_time
    
    # –í—ã—á–∏—Å–ª—è–µ–º —Ä–µ–π—Ç–∏–Ω–≥–∏
    ratings = runner.calculate_model_ratings(benchmark_results)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    results_file, report_file = runner.save_results(benchmark_results, ratings)
    
    print(f"\n‚úÖ –ë–µ–Ω—á–º–∞—Ä–∫ –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {total_time:.1f} —Å–µ–∫—É–Ω–¥")
    print(f"üìÑ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {results_file}")
    print(f"üìã –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {report_file}")
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ø-3
    print("\nüèÜ –¢–û–ü-3 –ú–û–î–ï–õ–ò:")
    for i, rating in enumerate(ratings[:3]):
        print(f"{i+1}. {rating.model_name} - {rating.overall_score:.1f} –æ—á–∫–æ–≤")

if __name__ == "__main__":
    asyncio.run(main()) 